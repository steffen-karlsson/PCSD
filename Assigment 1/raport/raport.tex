\documentclass[10pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\newcommand{\folge}[1]{\left \lbrace #1 \right \rbrace }
\lstset{language=Java, numbers=left, numberstyle=\footnotesize}
\author{Thorbj√∏rn Christensen \\
Steffen Karlsson \\
Kai Ejler Rasmussen}
\title{Principles of Computer System Design - Assignment 1}
\begin{document}
\maketitle

\section*{Exercises}
\subsection*{Question 1: Fundamental Abstractions}
\begin{enumerate}
	\item 
	We split the address used in READ and WRITE into a group of X least significant bits and a group of $Bits_{max}-X$ bits. The most significant group will refer to a machine using a name resolution service, in the client, and the least significant group will refer to a memory address on the specified server. Each server is added sequentially in the bit domain to allow for a sequential memory domain. The scalability of this solution depends on the distribution between the two groups in the naming. A possible distribution in a 64 bit domain could be $2^{56_{bits}}$ memory addresses and $2^{8_{bits}}$ possible machines. 
	
	This design, if configured correctly, will automatically change server when the address rolls over in the most significant group.
	
	
	\item The READ and WRITE API are both using a name resolution service (ie. lookup table or other service) to resolve the ip of the receiving server. The value is then read or written using RPC.
	\\
\begin{lstlisting}
READ(name)
  ip <- lookupServer(name & SERVER_BITMASK)
  memory <- name & MEMORY_BITMASK
  return getValueFromServer(ip, memory)
\end{lstlisting}	

\begin{lstlisting}
WRITE(name,value)
  ip <- lookupServer(name & SERVER_BITMASK)
  memory <- name & MEMORY_BITMASK
  writeValueToServer(ip, memory, value)
\end{lstlisting}
	
	\item The READ/WRITE API of the abstraction layer should be atomic to keep consistency, since generic READ/WRITE operations are atomic. To achieve this, a lock is obtained during the operations on the remote machine.
	
	\item The design allows for dynamic joins and leaves as long as the sequence is intact. If the servers 1,2 and 3 is available, a fourth server can be added as 00000100 (big-endian) in a byte to allow for roll over. The design does, however, not allow for serves to be added or removed out of sequence. For example removing server 2 gives the invalid sequence $1,3,4$. To accommodate for this, a centralized service layer could be added to reallocate memory addresses to similar to how bad blocks are reallocated on persistent storage.
	
	
\end{enumerate}

\subsection*{Question 2: Techniques for Performance}
\begin{enumerate}
	\item 
	Concurrency can reduce latency by splitting a request into several subtasks. In theory this speeds up the processing by a factor of $n$, where $n$ is the number of subtasks in parallel. However, there is still some overhead due to the splitting and serialization of collecting the results again which slows the concurrency. This overhead should be trivial for large requests compared to the performance gained.
	
	Concurrency increases performance if the task allows for it, however, it is difficult to implement and test concurrency. Concurrency can also decrease performance in cases where the overhead takes up more computation time than the actual request.
	
	\item
	Batching removes overhead from several requests by grouping them and processing them within the same request. This can be used to group read/write request to harddrive and minimize the movement of the disk arm by reordering the requests.
	
	Dallying is the concept of delaying requests, thus increasing latency, for the chance that the request won't be needed. For example, delaying a write request expecting a newer request to overwrite it will allow for deletion of the first request. This is also known as write absorption. Removing requests in dallying should make up for the latency it imposes in order to increase performance. 
	
	\item
	Having few central servers can give increased latency around the world, however, introducing geographically close caching servers solves this. These caching servers will act as a fast path for the most common requests, but cache misses will cause the request to contact the central servers, ie. the slow path, through the caching servers.
	
\end{enumerate}

\section*{Questions for Discussion on Architecture}

\end{document}

